{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/golfecholima/uscp_arrests/blob/master/uscp_arrests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-k3Crh3X90F"
   },
   "source": [
    "# Downloading and parsing US Capitol Police arrest reports\n",
    "\n",
    "Thanks to Mike Stucka for this great tutorial\n",
    "[this great tutorial on scraping with pyquery](https://github.com/PalmBeachPost/nicar19scraping/blob/master/00-Scraping%20--%20full%20self-tutorial.ipynb) ... yeah, I switched to bs4 but this got me started.\n",
    "\n",
    "### Things used:\n",
    "* [requests](https://2.python-requests.org/en/master/)\n",
    "* [pdfplumber](https://github.com/jsvine/pdfplumber)\n",
    "* [pandas](https://pandas.pydata.org)\n",
    "* [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [datasette/csvs-to-sqlite](https://datasette.io)\n",
    "* Logger from [@ilanamarcus](https://github.com/ilanamarcus)\n",
    "\n",
    "### Here's where the arrest reports [live](https://www.uscp.gov/media-center/weekly-arrest-summary).\n",
    "\n",
    "### To do:\n",
    "* ~~Refine datasette:~~\n",
    "    * ~~SQLite apparently infers that number an int, should stay string in case of leading zeros. (This apparently might not be possible.) Fixed w/ --shape.~~\n",
    "* ~~Functionify dir creation~~\n",
    "* ~~Make a new csv each time the script runs~~\n",
    "* Implement emails w/ function\n",
    "* YAML/cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2OykQnyVGUC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/site-packages (0.5.12)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: csvs-to-sqlite in /usr/local/lib/python3.7/site-packages (0.9)\n",
      "Requirement already satisfied: datasette in /usr/local/lib/python3.7/site-packages (0.28)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (4.8.0)\n",
      "Requirement already satisfied: pillow>=3.0.0 in /usr/local/lib/python3.7/site-packages (from pdfplumber) (6.1.0)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.7/site-packages (from pdfplumber) (0.14.1)\n",
      "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/site-packages (from pdfplumber) (3.8.2)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.7/site-packages (from pdfplumber) (3.0.4)\n",
      "Requirement already satisfied: wand in /usr/local/lib/python3.7/site-packages (from pdfplumber) (0.5.5)\n",
      "Requirement already satisfied: pdfminer.six==20181108 in /usr/local/lib/python3.7/site-packages (from pdfplumber) (20181108)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: click>=6.0.0 in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (7.0)\n",
      "Requirement already satisfied: py-lru-cache==0.1.4 in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (0.1.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (1.12.0)\n",
      "Requirement already satisfied: dateparser>=0.7.0 in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (0.7.1)\n",
      "Requirement already satisfied: click-default-group==1.2 in /usr/local/lib/python3.7/site-packages (from datasette) (1.2)\n",
      "Requirement already satisfied: Sanic==0.7.0 in /usr/local/lib/python3.7/site-packages (from datasette) (0.7.0)\n",
      "Requirement already satisfied: pint==0.8.1 in /usr/local/lib/python3.7/site-packages (from datasette) (0.8.1)\n",
      "Requirement already satisfied: pluggy>=0.7.1 in /usr/local/lib/python3.7/site-packages (from datasette) (0.12.0)\n",
      "Requirement already satisfied: hupper==1.0 in /usr/local/lib/python3.7/site-packages (from datasette) (1.0)\n",
      "Requirement already satisfied: Jinja2==2.10.1 in /usr/local/lib/python3.7/site-packages (from datasette) (2.10.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4) (1.9.3)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/site-packages (from pdfminer.six==20181108->pdfplumber) (2.1.0)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/site-packages (from dateparser>=0.7.0->csvs-to-sqlite) (1.5.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/site-packages (from dateparser>=0.7.0->csvs-to-sqlite) (2019.6.2)\n",
      "Requirement already satisfied: websockets>=4.0 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (7.0)\n",
      "Requirement already satisfied: aiofiles>=0.3.0 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (0.4.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (1.35)\n",
      "Requirement already satisfied: httptools>=0.0.9 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (0.0.13)\n",
      "Requirement already satisfied: uvloop>=0.5.3 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (0.12.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/site-packages (from pluggy>=0.7.1->datasette) (0.17)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/site-packages (from Jinja2==2.10.1->datasette) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12->pluggy>=0.7.1->datasette) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip3 install pdfplumber pandas requests csvs-to-sqlite datasette beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m9qVmGOVMIm"
   },
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Logger import Log\n",
    "\n",
    "# Built-in dependencies\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import subprocess\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "# Set up logging\n",
    "log = Log().getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o0-Auap8f-"
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.uscp.gov'\n",
    "url = base_url + '/media-center/weekly-arrest-summary'\n",
    "dt = str(datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "wd = os.getcwd()\n",
    "\n",
    "# Making directories\n",
    "def mkdir(d):\n",
    "    log.debug(f'Creating the directory {d} ...')\n",
    "    \n",
    "    if os.path.isdir(d):\n",
    "        log.debug(f'Directory already exists.')\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(d)\n",
    "        except OSError as e:\n",
    "            log.error(e)\n",
    "            log.debug(f'Failed.')\n",
    "        else:\n",
    "            log.debug(f'Success.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1inO1q7zNYR"
   },
   "outputs": [],
   "source": [
    "# Get the HTML and download the PDFs \n",
    "def download(site):\n",
    "    \n",
    "    reports = wd + '/reports'\n",
    "    \n",
    "    mkdir(reports)\n",
    "    \n",
    "    try:\n",
    "        log.debug(f'Getting {site}.')\n",
    "        html = urlopen(site)\n",
    "    except HTTPError as e:\n",
    "        log.error(e)\n",
    "        log.debug('Failed.')\n",
    "        # Send a message somewhere\n",
    "    except URLError as e:\n",
    "        log.error(e)\n",
    "        log.debug('Failed.')\n",
    "        # Send a message somewhere\n",
    "    else:\n",
    "        log.debug('Done.')\n",
    "    \n",
    "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "    links = bs.find_all('a', text= re.compile('Arrest Summary .+'))\n",
    "    \n",
    "    if not links:\n",
    "        log.debug('No links found. Hmmm, maybe the URL changed ...')\n",
    "        # This because bad URL slug ending still returned a page, just not the right one\n",
    "        # Send a message somewhere\n",
    "    else:\n",
    "        for link in links:\n",
    "            try:\n",
    "                href = link.attrs['href']\n",
    "            except AttributeError as e:\n",
    "                log.error(e)\n",
    "                log.debug('Failed.')\n",
    "                # Send a message somewhere\n",
    "            else:\n",
    "                filename = '/' + href.rsplit('/', 1)[1].lower().replace('%20', '_')\n",
    "                log.debug('Downloading ' + filename)\n",
    "                urllib.request.urlretrieve(base_url + href, reports + filename)\n",
    "    \n",
    "    pdfs = glob.glob(reports + '/*')\n",
    "    return pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U4YBhiXp1ZR"
   },
   "outputs": [],
   "source": [
    "# Parse the downloaded PDFs\n",
    "def parse_pdf(pdfs):\n",
    "    log.debug('Parsing the PDFs ...')\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for pdf in pdfs:\n",
    "        plumb = pdfplumber.open(pdf)\n",
    "        pages = plumb.pages # A list of PDF page objects\n",
    "        pages_text = ''\n",
    "\n",
    "        for page in pages:\n",
    "            text = page.extract_text()\n",
    "            pages_text += text\n",
    "\n",
    "        pages_text = re.sub(r'(^\\d\\s*(\\n|$))', '\\n', pages_text, flags=re.M) # Get rid of the page numbers\n",
    "\n",
    "        # Regex to find each arrest report chunk https://regex101.com/r/kWkaLi/7\n",
    "        regex = (\n",
    "                r'((?:(?:.+\\n)(?=(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12}))))'\n",
    "                r'(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12}))'\n",
    "                r'(?:(?:[\\s\\S]+?(?=(?:\\Z)|(?:(?:(?:.+\\n)(?=(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12})))))))))'\n",
    "        )\n",
    "\n",
    "        chunks = re.findall(regex, pages_text, flags=re.M)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            rows.append(chunk)\n",
    "    \n",
    "    log.debug('Done.')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f9-PUB0zNYU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse each chunk/row to cols then dataframe\n",
    "def parse_row(rows):\n",
    "    log.debug('Parsing each row into columns ...')\n",
    "    title = []\n",
    "    date = []\n",
    "    time = []\n",
    "    number = []\n",
    "    narrative = []\n",
    "    d = {\n",
    "    'title': title,\n",
    "    'date': date,\n",
    "    'time': time,\n",
    "    'number': number,\n",
    "    'narrative': narrative\n",
    "    }\n",
    "    \n",
    "    for row in rows:\n",
    "        row = row.strip() # Remove leading and trailing whitespace\n",
    "\n",
    "        # Regex to slice up the different data points of each 'chunk'\n",
    "        regex = r'(^.+\\n)(?:(\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(\\d{1,2}:\\d{1,2})(?:\\s+)(\\d{5,12}))([\\s\\S]+)'\n",
    "\n",
    "        titles = re.search(regex, row).group(1).strip()\n",
    "        dates = re.search(regex, row).group(2).strip()\n",
    "        times = re.search(regex, row).group(3).strip()\n",
    "        numbers = re.search(regex, row).group(4).strip()\n",
    "        narratives = re.sub('\\n', '',(re.search(regex, row).group(5).strip()))\n",
    "\n",
    "        title.append(titles)\n",
    "        date.append(dates)\n",
    "        time.append(times)\n",
    "        number.append(numbers)\n",
    "        narrative.append(narratives)\n",
    "    \n",
    "    log.debug('... putting into dataframe ...')\n",
    "    \n",
    "    df = pd.DataFrame(data = d)\n",
    "    \n",
    "    df['datetime'] = df['date'].map(str) + ' ' + df['time'] # Merge the date and time columns\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format = True) # Make that new column a datetime type\n",
    "    df['date'] = df['datetime'].dt.date # Split off date\n",
    "    df['time'] = df['datetime'].dt.time # Split off time\n",
    "    \n",
    "    log.debug('Done.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rd56g1voOejG"
   },
   "outputs": [],
   "source": [
    "# Make a CSV with datetime label that goes in the csv dir\n",
    "def mkcsv(df):\n",
    "    log.debug('Converting dataframe to csv ...')\n",
    "    csvs = wd + '/csv'\n",
    "    csv = csvs + '/uscp_arrests_' + dt + '.csv'\n",
    "    \n",
    "    mkdir(csvs)\n",
    "\n",
    "    df.to_csv(csv, encoding='utf-8', index=False)\n",
    "    log.debug(f'Saved the file {csv}.')\n",
    "    \n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ud1PQ4L7_vCY"
   },
   "outputs": [],
   "source": [
    "# Put it all in a datasette and 'publish'\n",
    "def ds(csv):\n",
    "    log.debug(f'Converting {csv} to .db ...')\n",
    "    dbs = wd + '/db'\n",
    "    db = dbs + '/uscp_arrests.db'\n",
    "    \n",
    "    mkdir(dbs)\n",
    "    \n",
    "    # Running terminal commands from python\n",
    "    subprocess.check_call([\n",
    "        'csvs-to-sqlite',\n",
    "        '--replace-tables',\n",
    "        '--shape',\n",
    "        'title:title,date:date(TEXT),time:time(TEXT),number:number(TEXT),narrative:narrative,datetime:datetime(TEXT)',\n",
    "        csv,\n",
    "        db]) \n",
    "    # ^^ Trixy ^^ any time you would have a space in the command line\n",
    "    # you need to comma separate and have a news string in the brackets.\n",
    "\n",
    "    log.debug('Starting datasette at http://127.0.0.1:8001/ ...')\n",
    "    subprocess.check_call(['datasette', db])\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYSxwfKHzNYZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-20 18:34:26,125 - DEBUG - Creating the directory /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests/reports ...\n",
      "2019-09-20 18:34:26,128 - DEBUG - Directory already exists.\n",
      "2019-09-20 18:34:26,130 - DEBUG - Getting https://www.uscp.gov/media-center/weekly-arrest-summary.\n",
      "2019-09-20 18:34:26,235 - DEBUG - Done.\n",
      "2019-09-20 18:34:26,266 - DEBUG - Downloading /arrest_summary_12-20-18_1-2-19_1.pdf\n",
      "2019-09-20 18:34:26,332 - DEBUG - Downloading /arrest_summary_1-3-19_1-9-19.pdf\n",
      "2019-09-20 18:34:26,406 - DEBUG - Downloading /arrest_summary_1-10-19_1-16-19.pdf\n",
      "2019-09-20 18:34:26,467 - DEBUG - Downloading /arrest_summary_1-17-19_1-23-19.pdf\n",
      "2019-09-20 18:34:26,561 - DEBUG - Downloading /arrest_summary_1-24-19_1-30-19.pdf\n",
      "2019-09-20 18:34:26,626 - DEBUG - Downloading /arrest_summary_1-31-19_2-6-19.pdf\n",
      "2019-09-20 18:34:26,689 - DEBUG - Downloading /arrest_summary_2-7-19_2-13-19.pdf\n",
      "2019-09-20 18:34:26,747 - DEBUG - Downloading /arrest_summary_2-14-19_2-27-19.pdf\n",
      "2019-09-20 18:34:26,811 - DEBUG - Downloading /arrest_summary_2-28-19_3-6-19.pdf\n",
      "2019-09-20 18:34:26,875 - DEBUG - Downloading /arrest_summary_3-7-19_3-13-19.pdf\n",
      "2019-09-20 18:34:26,940 - DEBUG - Downloading /arrest_summary_3-14-19_3-20-19.pdf\n",
      "2019-09-20 18:34:27,006 - DEBUG - Downloading /arrest_summary_3-21-19_3-27-19.pdf\n",
      "2019-09-20 18:34:27,072 - DEBUG - Downloading /arrest_summary_3-28-19_4-3-19.pdf\n",
      "2019-09-20 18:34:27,140 - DEBUG - Downloading /arrest_summary_4-4-19_4-10-19_0.pdf\n",
      "2019-09-20 18:34:27,229 - DEBUG - Downloading /arrest_summary_4-11-19_4-17-19_1.pdf\n",
      "2019-09-20 18:34:27,311 - DEBUG - Downloading /arrest_summary_4-18-19_4-24-19_0.pdf\n",
      "2019-09-20 18:34:27,405 - DEBUG - Downloading /arrest_summary_4-25-19_5-1-19_0.pdf\n",
      "2019-09-20 18:34:27,485 - DEBUG - Downloading /arrest_summary_5-2-19_5-8-19_0.pdf\n",
      "2019-09-20 18:34:27,562 - DEBUG - Downloading /arrest_summary_5-9-19_5-15-19_0.pdf\n",
      "2019-09-20 18:34:27,637 - DEBUG - Downloading /arrest_summary_5-16-19_5-22-19_2.pdf\n",
      "2019-09-20 18:34:27,704 - DEBUG - Downloading /arrest_summary_5-23-19_5-29-19_0.pdf\n",
      "2019-09-20 18:34:27,785 - DEBUG - Downloading /arrest_summary_5-30-19_6-5-19_0.pdf\n",
      "2019-09-20 18:34:27,855 - DEBUG - Downloading /arrest_summary_6-6-19_6-12-19_0.pdf\n",
      "2019-09-20 18:34:27,932 - DEBUG - Downloading /arrest_summary_6-13-19_6-19-19_1.pdf\n",
      "2019-09-20 18:34:28,015 - DEBUG - Downloading /arrest_summary_6-20-19_6-26-19_0.pdf\n",
      "2019-09-20 18:34:28,096 - DEBUG - Downloading /arrest_summary_6-27-19_7-3-19.pdf\n",
      "2019-09-20 18:34:28,172 - DEBUG - Downloading /arrest_summary_7-4-19_7-10-19.pdf\n",
      "2019-09-20 18:34:28,246 - DEBUG - Downloading /arrest_summary_7-11-19_7-17-19.pdf\n",
      "2019-09-20 18:34:28,318 - DEBUG - Downloading /arrest_summary_7-18-19_7-24-19.pdf\n",
      "2019-09-20 18:34:28,395 - DEBUG - Downloading /arrest_summary_7-25-19_7-31-19.pdf\n",
      "2019-09-20 18:34:28,472 - DEBUG - Downloading /arrest_summary_8-1-19_8-7-19.pdf\n",
      "2019-09-20 18:34:28,554 - DEBUG - Downloading /arrest_summary_8-8-19_8-14-19.pdf\n",
      "2019-09-20 18:34:28,626 - DEBUG - Downloading /arrest_summary_8-15-19_8-21-19.pdf\n",
      "2019-09-20 18:34:28,693 - DEBUG - Downloading /arrest_summary_8-22-19_8-28-19.pdf\n",
      "2019-09-20 18:34:28,771 - DEBUG - Downloading /arrest_summary_8-29-19_9-4-19.pdf\n",
      "2019-09-20 18:34:28,846 - DEBUG - Downloading /arrest_summary_9-5-19_9-11-19.pdf\n",
      "2019-09-20 18:34:28,926 - DEBUG - Downloading /arrest_summary_9-12-19_9-18-19_0.pdf\n",
      "2019-09-20 18:34:29,016 - DEBUG - Parsing the PDFs ...\n",
      "2019-09-20 18:34:51,448 - DEBUG - Done.\n",
      "2019-09-20 18:34:51,449 - DEBUG - Parsing each row into columns ...\n",
      "2019-09-20 18:34:51,470 - DEBUG - ... putting into dataframe ...\n",
      "2019-09-20 18:34:51,488 - DEBUG - Done.\n",
      "2019-09-20 18:34:51,490 - DEBUG - Converting dataframe to csv ...\n",
      "2019-09-20 18:34:51,492 - DEBUG - Creating the directory /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests/csv ...\n",
      "2019-09-20 18:34:51,501 - DEBUG - Success.\n",
      "2019-09-20 18:34:51,533 - DEBUG - Saved the file /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests/csv/uscp_arrests_20190920_183426.csv.\n",
      "2019-09-20 18:34:51,538 - DEBUG - Converting /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests/csv/uscp_arrests_20190920_183426.csv to .db ...\n",
      "2019-09-20 18:34:51,541 - DEBUG - Creating the directory /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests/db ...\n",
      "2019-09-20 18:34:51,551 - DEBUG - Success.\n",
      "2019-09-20 18:34:52,572 - DEBUG - Starting datasette at http://127.0.0.1:8001/ ...\n"
     ]
    }
   ],
   "source": [
    "# Do the things\n",
    "\n",
    "pdfs = download(url)\n",
    "rows = parse_pdf(pdfs)\n",
    "df = parse_row(rows)\n",
    "csv = mkcsv(df)\n",
    "ds(csv)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "uscp_arrests.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
