{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-k3Crh3X90F"
   },
   "source": [
    "# Downloading and parsing US Capitol Police arrest reports\n",
    "\n",
    "Thanks to Mike Stucka for this great tutorial\n",
    "[this great tutorial on scraping with pyquery](https://github.com/PalmBeachPost/nicar19scraping/blob/master/00-Scraping%20--%20full%20self-tutorial.ipynb) ... yeah, I switched to bs4 but this got me started.\n",
    "\n",
    "### Things used:\n",
    "* [requests](https://2.python-requests.org/en/master/)\n",
    "* [pdfplumber](https://github.com/jsvine/pdfplumber)\n",
    "* [pandas](https://pandas.pydata.org)\n",
    "* [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [datasette/csvs-to-sqlite](https://datasette.io)\n",
    "* Error logger from @coworker_whose_github_i_couldnt_find\n",
    "* \n",
    "\n",
    "### Here's where the arrest reports [live](https://www.uscp.gov/media-center/weekly-arrest-summary).\n",
    "\n",
    "### To do:\n",
    "* ~~Refine datasette:~~\n",
    "    * ~~SQLite apparently infers that number an int, should stay string in case of leading zeros. (This apparently might not be possible.) Fixed w/ --shape.~~\n",
    "* ~~Functionify dir creation~~\n",
    "* ~~Make a new csv each time the script runs~~\n",
    "* Implement emails w/ function\n",
    "* YAML/cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2OykQnyVGUC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/site-packages (0.5.12)\n",
      "Requirement already satisfied: pyquery in /usr/local/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (1.16.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: csvs-to-sqlite in /usr/local/lib/python3.7/site-packages (0.9)\n",
      "Requirement already satisfied: datasette in /usr/local/lib/python3.7/site-packages (0.28)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (4.8.0)\n",
      "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/site-packages (from pdfplumber) (3.8.2)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.7/site-packages (from pdfplumber) (0.14.1)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.7/site-packages (from pdfplumber) (3.0.4)\n",
      "Requirement already satisfied: pdfminer.six==20181108 in /usr/local/lib/python3.7/site-packages (from pdfplumber) (20181108)\n",
      "Requirement already satisfied: pillow>=3.0.0 in /usr/local/lib/python3.7/site-packages (from pdfplumber) (6.1.0)\n",
      "Requirement already satisfied: wand in /usr/local/lib/python3.7/site-packages (from pdfplumber) (0.5.5)\n",
      "Requirement already satisfied: cssselect>0.7.9 in /usr/local/lib/python3.7/site-packages (from pyquery) (1.0.3)\n",
      "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.7/site-packages (from pyquery) (4.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: click>=6.0.0 in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (7.0)\n",
      "Requirement already satisfied: dateparser>=0.7.0 in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (0.7.1)\n",
      "Requirement already satisfied: py-lru-cache==0.1.4 in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (0.1.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from csvs-to-sqlite) (1.12.0)\n",
      "Requirement already satisfied: Jinja2==2.10.1 in /usr/local/lib/python3.7/site-packages (from datasette) (2.10.1)\n",
      "Requirement already satisfied: pint==0.8.1 in /usr/local/lib/python3.7/site-packages (from datasette) (0.8.1)\n",
      "Requirement already satisfied: pluggy>=0.7.1 in /usr/local/lib/python3.7/site-packages (from datasette) (0.12.0)\n",
      "Requirement already satisfied: click-default-group==1.2 in /usr/local/lib/python3.7/site-packages (from datasette) (1.2)\n",
      "Requirement already satisfied: hupper==1.0 in /usr/local/lib/python3.7/site-packages (from datasette) (1.0)\n",
      "Requirement already satisfied: Sanic==0.7.0 in /usr/local/lib/python3.7/site-packages (from datasette) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4) (1.9.3)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/site-packages (from pdfminer.six==20181108->pdfplumber) (2.1.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/site-packages (from dateparser>=0.7.0->csvs-to-sqlite) (2019.6.2)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/site-packages (from dateparser>=0.7.0->csvs-to-sqlite) (1.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/site-packages (from Jinja2==2.10.1->datasette) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/site-packages (from pluggy>=0.7.1->datasette) (0.17)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (1.35)\n",
      "Requirement already satisfied: uvloop>=0.5.3 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (0.12.2)\n",
      "Requirement already satisfied: httptools>=0.0.9 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (0.0.13)\n",
      "Requirement already satisfied: websockets>=4.0 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (7.0)\n",
      "Requirement already satisfied: aiofiles>=0.3.0 in /usr/local/lib/python3.7/site-packages (from Sanic==0.7.0->datasette) (0.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12->pluggy>=0.7.1->datasette) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip3 install pdfplumber pyquery numpy pandas requests csvs-to-sqlite datasette beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m9qVmGOVMIm"
   },
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Logger import Log\n",
    "\n",
    "# Built-in dependencies\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import subprocess\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "9WLNrzglBQzi",
    "outputId": "970fbe5c-27b5-4c12-d12e-8912b6a4dbe1"
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "log = Log().getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o0-Auap8f-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests.\n",
      "Successfully created the directory /reports.\n",
      "The current working directory is /Volumes/GoogleDrive/My Drive/data/ipynbs/uscp_arrests.\n",
      "The directory /debug already exists.\n"
     ]
    }
   ],
   "source": [
    "# Set up some file structure to put PDFs and debug logs in\n",
    "\n",
    "reports_dir = '/reports'\n",
    "debug_dir = '/debug'\n",
    "\n",
    "# Little function for making directories\n",
    "def mkdir(d):\n",
    "    wd = os.getcwd()\n",
    "    print(f'The current working directory is {wd}.')\n",
    "\n",
    "    if os.path.isdir(wd + d):\n",
    "        print(f'The directory {d} already exists.')\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(wd + d)\n",
    "        except OSError as e:\n",
    "            log.debug(e)\n",
    "            print(f\"Creation of the directory {d} failed.\")\n",
    "        else:\n",
    "            print(f\"Successfully created the directory {d}.\")\n",
    "\n",
    "mkdir(reports_dir)\n",
    "mkdir(debug_dir)\n",
    "\n",
    "# wd = os.getcwd()\n",
    "# print(f'The current working directory is {wd}.')\n",
    "\n",
    "# reports_dir = '/reports'\n",
    "\n",
    "# if os.path.isdir(wd + reports_dir):\n",
    "#     print(f'The directory {reports_dir} already exists.')\n",
    "# else:\n",
    "#     try:\n",
    "#         os.mkdir(wd + reports_dir)\n",
    "#     except OSError as e:\n",
    "#         log.debug(e)\n",
    "#         print(f\"Creation of the directory {reports_dir} failed.\")\n",
    "#     else:\n",
    "#         print(f\"Successfully created the directory {reports_dir}.\")\n",
    "\n",
    "# debug_dir = '/debug'\n",
    "\n",
    "# if os.path.isdir(wd + debug_dir):\n",
    "#     print(f'The directory {debug_dir} already exists.')\n",
    "# else:\n",
    "#     try:\n",
    "#         os.mkdir(wd + debug_dir)\n",
    "#     except OSError as e:\n",
    "#         log.debug(e)\n",
    "#         print(f\"Creation of the directory {debug_dir} failed.\")\n",
    "#     else:\n",
    "#         print(f\"Successfully created the directory {debug_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U4YBhiXp1ZR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, https://www.uscp.gov/media-center/weekly-arrest-summary fetched.\n",
      "Downloading /arrest_summary_12-20-18_1-2-19_1.pdf\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-06bda07a3a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreports_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wd' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the HTML for the page w/ some error handling\n",
    "base_url = 'https://www.uscp.gov'\n",
    "url = 'https://www.uscp.gov/media-center/weekly-arrest-summary'\n",
    "\n",
    "try: \n",
    "    html = urlopen(url)\n",
    "except HTTPError as e:\n",
    "    log.debug(e)\n",
    "    # Send a message somewhere\n",
    "except URLError as e:\n",
    "    log.debug(e)\n",
    "    # Send a message somewhere\n",
    "else:\n",
    "    print(f'Success, {url} fetched.')\n",
    "\n",
    "# Get the arrest report links and download the PDFs to the created directory\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "link_list = bs.find_all('a', text= re.compile('Arrest Summary .+'))\n",
    "\n",
    "if not link_list:\n",
    "    print('No links found. Hmmm, maybe the URL changed ...') \n",
    "    # Saying this because bad URL slug ending still returned a page, just not the right one\n",
    "    # Send a message somewhere\n",
    "else:\n",
    "    for link in link_list:\n",
    "        try:\n",
    "            href = link.attrs['href']\n",
    "        except AttributeError as e:\n",
    "            log.debug(e)\n",
    "            # Send a message somewhere\n",
    "        else:\n",
    "            filename = '/' + href.rsplit('/', 1)[1].lower().replace('%20', '_')\n",
    "            print('Downloading ' + filename)\n",
    "            urllib.request.urlretrieve(base_url + href, wd + reports_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rd56g1voOejG"
   },
   "outputs": [],
   "source": [
    "# Parse the downloaded PDFs\n",
    "pdfs = glob.glob(wd + reports_dir + '/*')\n",
    "\n",
    "report_list = []\n",
    "\n",
    "for pdf in pdfs:\n",
    "    plumb = pdfplumber.open(pdf)\n",
    "    pages = plumb.pages # A list of PDF page objects\n",
    "    \n",
    "    pages_text = ''\n",
    "\n",
    "    for page in pages:\n",
    "        text = page.extract_text()\n",
    "        pages_text += text\n",
    "    \n",
    "    pages_text = re.sub(r'(^\\d\\s*(\\n|$))', '\\n', pages_text, flags=re.M) # Get rid of the page numbers\n",
    "    \n",
    "    \n",
    "    # Regex to find each arrest report chunk https://regex101.com/r/kWkaLi/7\n",
    "    chunk = (\n",
    "            r'((?:(?:.+\\n)(?=(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12}))))'\n",
    "            r'(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12}))'\n",
    "            r'(?:(?:[\\s\\S]+?(?=(?:\\Z)|(?:(?:(?:.+\\n)(?=(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12})))))))))'\n",
    "    )\n",
    "    \n",
    "    reports = re.findall(chunk, pages_text, flags=re.M)\n",
    "    \n",
    "    for report in reports:\n",
    "        report_list.append(report)\n",
    "    \n",
    "title = []\n",
    "date = []\n",
    "time = []\n",
    "number = []\n",
    "narrative = []\n",
    "\n",
    "for report in report_list:\n",
    "    report = report.strip() # Remove leading and trailing whitespace\n",
    "    \n",
    "    # Regex to slice up the different data points of each 'chunk'\n",
    "    regex = r'(^.+\\n)(?:(\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(\\d{1,2}:\\d{1,2})(?:\\s+)(\\d{5,12}))([\\s\\S]+)'\n",
    "\n",
    "    titles = re.search(regex, report).group(1).strip()\n",
    "    dates = re.search(regex, report).group(2).strip()\n",
    "    times = re.search(regex, report).group(3).strip()\n",
    "    numbers = re.search(regex, report).group(4).strip()\n",
    "    narratives = re.sub('\\n', '',(re.search(regex, report).group(5).strip()))\n",
    "\n",
    "    title.append(titles)\n",
    "    date.append(dates)\n",
    "    time.append(times)\n",
    "    number.append(numbers)\n",
    "    narrative.append(narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ud1PQ4L7_vCY"
   },
   "outputs": [],
   "source": [
    "# Put it all in a pandas dataframe \n",
    "d = {\n",
    "    'title': title,\n",
    "    'date': date,\n",
    "    'time': time,\n",
    "    'number': number,\n",
    "    'narrative': narrative\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "df['datetime'] = df['date'].map(str) + ' ' + df['time'] # Merge the date and time columns\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format = True) # Make that new column a datetime type\n",
    "df['date'] = df['datetime'].dt.date # Split off date\n",
    "df['time'] = df['datetime'].dt.time # Split off time\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qp4yKbHJFVRN"
   },
   "outputs": [],
   "source": [
    "# Make a CSV with datetime label that goes in the csv dir\n",
    "dt = str(datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "csv_dir = '/csv'\n",
    "csv_file = '/uscp_arrests_' + dt + '.csv'\n",
    "\n",
    "mkdir(csv_dir)\n",
    "\n",
    "df.to_csv(wd + csv_dir + csv_file, encoding='utf-8', index=False)\n",
    "print(f'Saved the file {csv_file} to {csv_dir}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all in a datasette and publish \n",
    "\n",
    "db_dir = '/db'\n",
    "csv = wd + csv_dir\n",
    "db = wd + db_dir + '/uscp_arrests.db'\n",
    "\n",
    "mkdir(db_dir)\n",
    "\n",
    "# Running terminal commands from python\n",
    "subprocess.check_call([\n",
    "    'csvs-to-sqlite',\n",
    "    '--replace-tables',\n",
    "    '--shape',\n",
    "    'title:title,date:date(TEXT),time:time(TEXT),number:number(TEXT),narrative:narrative,datetime:datetime(TEXT)',\n",
    "    csv,\n",
    "    db]) \n",
    "# ^^ Trixy ^^ any time you would have a space in the command line\n",
    "# you need to comma separate and have a news string in the brackets.\n",
    "\n",
    "print('Starting uscp_arrestes.db datasette at http://127.0.0.1:8001/ ...')\n",
    "subprocess.check_call(['datasette', db])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "uscp_arrests.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
